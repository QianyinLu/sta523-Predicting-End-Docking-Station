---
title: "Homework 6"
author: 'Steven Winter, Zining Ma, Qianyin Lu, Mengxuan Cui'
date: "11/14/2019"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      warning = FALSE)
```

## Task 1

Start by loading backages and the data.
```{r}
library(tidyverse)
library(vroom)
#devtools::install_github("tidyverse/multidplyr")
# make sure all other packages are up to date first
library(multidplyr)
library(lubridate)
library(data.table)
library(profvis)
```

Using the code from the slides: **Note - we might not want to use all of the data.**
```{r}
base_url = "http://www2.stat.duke.edu/~sms185/data/bike/"
files = c("cbs_2013.csv", "cbs_2014.csv", "cbs_2015.csv", "cbs_2016.csv", "cbs_2017.csv")

cbs_names = c("duration", "start_date", "end_date", "start_station_number", 
               "start_station","start_station_number", "start_station", 
               "bike_number","member_type")

# code from slides
clust <- multidplyr::new_cluster(3)
multidplyr::cluster_assign_partition(clust, file_name = paste0(base_url, files))
multidplyr::cluster_send(clust, cbs_data <- vroom::vroom(file_name))
cbs <- multidplyr::party_df(clust, "cbs_data")
```

Change the names with ``rename()`` to match what we'll see in the test set:
```{r}
cbs = cbs %>%
  dplyr::rename(duration = Duration,
         start_date = `Start date`,
         end_date = `End date`,
         start_station_number = `Start station number`,
         start_station = `Start station`,
         end_station_number = `End station number`,
         end_station = `End station`,
         member_type = `Member type`,
         bike_number = `Bike number`)
```

We'll add the month and year as a column to make sorting quicker later on. **Note - this is where we would add the weather.**
```{r}
cbs = cbs %>% mutate(year = lubridate::year(start_date),
                     month = lubridate::month(start_date),
                     day = lubridate::day(start_date),
                     hour = lubridate::hour(start_date),
                     # wday returns 2~6 for Monday~Friday
                     weekday = lubridate::wday(start_date) %in% 2:6)
```

Now we can safely move everything back to our machine, make a data.table and set keys. **It might make sense to set weather as the second key here too.**
```{r}
cbs_dt = data.table(collect(cbs))
```

Then we merge weather data. Script getting weather data and parse them into dataframe are in folder /get, due to api limitation, it needs at least 3 days to get all the data, so we just uploaded prepared weather.rds
```{r}
weather = readRDS("weather.rds")
weat_cond = weather %>% 
  select(year,month,day,hour,weekday,icon,temperature) %>% 
  #weekday 1~5 are Monday~Friday
  mutate(weekday = weekday %in% 1:5,
         icon = recode(icon,
                       `clear-day` = "nice",
                       `clear-night` = "nice",
                       `cloudy` = "nice",
                       `fog` = "bad",
                       `partly-cloudy-day` = "nice",
                       `partly-cloudy-night` = "nice",
                       `rain` = "terrible",
                       `sleet` = "terrible",
                       `snow` = "terrible"))

cbs_dt = merge(cbs_dt,weat_cond,all.x=T,sort = F)
```

```{r}
setkey(cbs_dt, start_station, duration, end_station)
```

## Task 2:

### Helper Functions
We have 2 hours and 4 cores to play with: that's 120x4x60=28800 seconds of computing time. There are 12532 rows we need to predict. This gives us 28800/12532 = 2.3 seconds per row, including overhead and everything. That means we probably need to get each individual prediction to under two seconds for this to work.

First a function for getting "similar" rows. We'll use data.tables because it's crazy fast. **This function needs to be generalized a little. Add weather, etc. Order really, really, really matters.**

```{r}
get_similar_rows <- function(row, input_dt, duration_minus = 0.2,duration_plus=0.2,
                             month_delta = 2, hour_delta=2, temp_delta = 10) {
  # filter the data.table to rows with:
  # - the same start station
  # - durations in [1-duration_delta, 1+duration_delta]*actual_duration
  # - months within +- month_delta
  # - hours withim +- hour delta
  
  
  same_start = cbs_dt[J(row$start_station), nomatch=0L]
  
  same_start[T
             & duration %between% c((1-duration_minus)*row$duration, (1+duration_plus)*row$duration)
             #& abs(month - row$month)<= month_delta
             & abs(hour-row$hour) <= hour_delta
             & weekday == row$weekday
             #& icon == row$icon
             #& abs(temperature - row$temperature) <= temp_delta
             ]
}
```

The next step is to assign the empirical probabilitiy to each row. Example: If there are 400 posible stops and stop j appears 18 times in the list of similar rows then we would set p(end_stop = stop_j|data) = 18/400/ If stop k does not appear in the similar rows then we set p(end_stop = stop_k|data) = 0. **Could also just try assigning a uniform probability to these.**
```{r}
make_predictions <- function(similar_rows, all_stops) {
  # takes in a list of similar rows
  # assigns each unique stop the observed probability
  # assigns any missing stops zero probability
  
  nonzero_probs = as.data.frame(table(similar_rows$end_station)/nrow(similar_rows))
  predictions = spread(nonzero_probs, key=Var1, value=Freq)
  
  zero_probs = all_stops[!(all_stops %in% names(predictions))]
  predictions[zero_probs] = 0
  
  predictions
}

```

**Also need a fast helper function to join these together. Maybe mcapply or something? I just have a simple wrapper for now. Sorting the columns too for faster binding later. Currently handling outliers by using the precomputed naive probability. We can do better.**

```{r}
prediction_wrapper = function(row, input_dt, features, all_stops, ordered_names) {
  # takes in a row from the test_set
  # computes and appends predictions
  
  similar_rows = get_similar_rows(row, input_dt)
  
  if(nrow(similar_rows)==0){
    # if there are are no similar rows in our data, just guess
    return(row[,ordered_names])
  }
  
  # compute and append predictions
  predictions = make_predictions(similar_rows, all_stops)
  output_row = cbind(row[features], predictions)
  
  # return everything in the same order to make rbind faster
  output_row[,ordered_names]
}
```


### Testing
Need to load the test data and include weather condition. **We should add weather to the test data in the next code chunk:**
```{r}
test_set = vroom::vroom("http://www2.stat.duke.edu/~sms185/data/bike/cbs_test.csv")
test_wet = test_set %>% mutate(year = lubridate::year(start_date),
                               month = lubridate::month(start_date),
                               day = lubridate::day(start_date),
                               hour = lubridate::hour(start_date),
                               # wday returns 2~6 for Monday~Friday
                               weekday = lubridate::wday(start_date) %in% 2:6)
test_wet = merge(test_wet,weat_cond,all.x = T,sort = F) %>% arrange(start_date)
```

Get extra information to pass to the helper functions:
```{r}
ordered_names = names(test_set)

exclude_features = c("year","month","day","hour","weekday","icon","temperature")
ordered_names = ordered_names[!(ordered_names %in% exclude_features)]
features = c("start_date","end_date","duration","start_station_number",
                  "start_station","bike_number", "member_type")
all_stops = names(test_set)[!(names(test_set) %in% c(features,exclude_features))]

```

Here's an example row:
```{r}
prediction_wrapper(test_wet[500,], cbs_dt, features, all_stops, ordered_names)
```

Finally, run our functions on the entire thing.

**Currently only running on part of the data.**

```{r}
final_predictions = plyr::adply(test_wet, 1, function(x) prediction_wrapper(x, cbs_dt, features, all_stops, ordered_names),
                           .progress = "text")
final_predictions = final_predictions %>% select(-exclude_features)
#write_csv(final_predictions, "cbs_git-r-done.csv")
```

Save these results:
```{r}
write_csv(final_predictions, "cbs_git-r-done.csv")
```

**This is writing the times incorrectly and adding row names...Maybe still has a month column too? Not sure how to fix this right now.**

## Model selection

In our final model, we use these conditions:

1. $|hour_{sample}-hour_{pred}|<=2$

2. $duration_{sample}\in[0.8*duration_{pred},1.2*duration_{pred}]$

3. $weekday_{sample}$ and $weekday_{pred}$ are both in {Monday~Friday} or {Sunday,Saturday}

How we choose our model is described detailed below:

We started from a model which considers condition $\Delta_{month}\leq2$, $\Delta_{hour}\leq2$, $\Delta_{duration}\leq20\%$, and this model gave us score=5.61.

Then we thought weather and weekday may affect people's choice, so we 

1. integrated weather data: temperature and icon, which has values {nice,bad,terrible}

2. add weekday indicator: with values TRUE: weekday is in Monday~Friday, and FALSE: weekday is Sunday or Saturday.

We combined temperature, icon and weekday into above setting, and got a terrible score=3.75.

Then we tried combinations of new variables and got following results:

1. weekday+temperature(change $\leq$ 7):5.23

2. weekday+icon:5.43

3. weekday only:5.77

(We didn't exclude weekday because we think it really matters)

It's a shock that adding new variables more than weekday is making the score worse. We guess the reason is that, adding variables is reducing our filtered sample too much that the given prediction can not capture the whole picture.

So we changed our direction: what will happed if we add very little conditions?

We tried two setting:

1. only weekday indicator, no month,hour,duration and weather, and got a score=1.97

2. no condition at all, completely naive prediction based on whole data, and got score 1.90

Comparision between these attempts and those above, it shows that duration and time really matters.

And then we chose a model as:

$\Delta_{duration}\leq20\%$, $\Delta_{hour}\leq2$, $\Delta{temperature}\leq7$ and consider weekday.

We have hour here but month not because We think time in a day is much more important than month, and information of month is mainly season, which could be replaced by temperature. This setting gave us a highest score 5.85 at that time.

Then based on this setting, we tried more adjustment:

1. change $\Delta_{duration}\leq20\%$ to $\Delta_{duration}\in[-50\%,20\%]$ and $\Delta_{duration}\in[-20\%,50\%]$ and got score 5.03 and 5.40 respectively. It shows that our previous setting $\Delta_{duration}\leq20\%$ is a good intuitive choice.

2. change $\Delta_{temperature}\leq7$ to $\Delta_{temperature}\leq5$ and $\Delta_{temperature}\leq10$ and got score 5.65 and 6.02 respectively. It shows that larger range of temperature give more precise prediction. 

3. so We tried next model excluding temperature, and got a amazing score 6.15.

This final model including conditions $\Delta_{duration}\leq20\%$, 
$\Delta_{hour}\leq2$ and weekday indicator is our best model. And the process we got here showes that time is a sigificant factor and weather doesn't matter.
