---
title: "Homework 6"
author: '[Group member names]'
date: "11/05/2019"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      warning = FALSE)
```

## Task 1

Start by loading backages and the data.
```{r}
library(tidyverse)
library(vroom)
#devtools::install_github("tidyverse/multidplyr")
# make sure all other packages are up to date first
library(multidplyr)
library(lubridate)
library(data.table)
```

**Currently only using 3 years of data and three clusters. Make sure to use four NEW clusters to parallelize the predictions.**

**TO DO - see if all the data performs better**
```{r}
base_url = "http://www2.stat.duke.edu/~sms185/data/bike/"
#files = c("cbs_2013.csv", "cbs_2014.csv", "cbs_2015.csv", "cbs_2016.csv", "cbs_2017.csv")
files = c("cbs_2015.csv", "cbs_2016.csv", "cbs_2017.csv")

cbs_names = c("duration", "start_date", "end_date", "start_station_number", 
               "start_station","start_station_number", "start_station", 
               "bike_number","member_type")

# code from slides
clust <- multidplyr::new_cluster(3)
multidplyr::cluster_assign_partition(clust, file_name = paste0(base_url, files))
multidplyr::cluster_send(clust, cbs_data <- vroom::vroom(file_name))
cbs <- multidplyr::party_df(clust, "cbs_data")
```

Change the names with ``rename()`` to match what we'll see in the test set:
```{r}
cbs = cbs %>%
  rename(duration = Duration,
         start_date = `Start date`,
         end_date = `End date`,
         start_station_number = `Start station number`,
         start_station = `Start station`,
         end_station_number = `End station number`,
         end_station = `End station`,
         member_type = `Member type`,
         bike_number = `Bike number`)
```

We'll add the month and year as a column to make sorting quicker later on. **Note - this is where we would add the weather.**
```{r}
cbs = cbs %>% mutate(month = lubridate::month(start_date),
               hour = lubridate::hour(start_date))
```

Now we can safely move everything back to our machine, make a data.table and set keys. **It might make sense to set weather as the second key here too.**
```{r}
cbs_dt = data.table(collect(cbs))
setkey(cbs_dt, start_station, duration, end_station)
```


## Task 2:

### Helper Functions
We have 2 hours and 4 cores to play with: that's 120x4x60=28800 seconds of computing time. There are 12532 rows we need to predict. This gives us 28800/12532 = 2.3 seconds per row, including overhead and everything. That means we probably need to get each individual prediction to under two seconds for this to work.

First a function for getting "similar" rows. We'll use data.tables because it's crazy fast. **This function needs to be generalized a little. Add weather, etc. Order really, really, really matters.**
```{r}
get_similar_rows <- function(row, input_dt) {
  
  same_start = cbs_dt[J(row$start_station), nomatch=0L]
  
  same_start[duration %between% c(0.8*row$duration, 1.2*row$duration) &
               abs(month(start_date) - month(row$start_date))<=3 &
               abs(hour(start_date))-hour(row$start_date) <=3]
}
```

The next step is to assign the empirical probabilitiy to each row. Example: If there are 400 posible stops and stop j appears 18 times in the list of similar rows then we would set p(end_stop = stop_j|data) = 18/400/ If stop k does not appear in the similar rows then we set p(end_stop = stop_k|data) = 0. **Needs comments.**
```{r}
make_predictions <- function(similar_rows, all_stops) {
  
  nonzero_probs = as.data.frame(table(similar_rows$end_station)/nrow(similar_rows))
  predictions = spread(nonzero_probs, key=Var1, value=Freq)
  
  zero_probs = all_stops[!(all_stops %in% names(predictions))]
  predictions[zero_probs] = 0
  
  predictions
}

```

**Also need a fast helper function to join these together. Maybe mcapply or something? I just have a simple wrapper for now. Sorting the columns too for faster binding later. Currently handling outliers by using the precomputed naive probability. We can do better.**

```{r}
prediction_wrapper = function(row, input_dt, features, all_stops, ordered_names) {
  
  similar_rows = get_similar_rows(row, input_dt)
  
  # if nrow(similar_rows) is zero, do something else
  # could probably do better than this
  if(nrow(similar_rows)==0){
    return(row[,ordered_names])
  }
  
  # maybe give these different names
  predictions = make_predictions(similar_rows, all_stops)
  
  predictions = cbind(row[features], predictions)
  
  predictions[,ordered_names]
}
```


### Testing
Need to load the test data. **We should add weather to the test data in the next code chunk:**
```{r}
test_set = vroom::vroom("http://www2.stat.duke.edu/~sms185/data/bike/cbs_test.csv")
test_set = test_set %>% mutate(month = lubridate::month(start_date),
               hour = lubridate::hour(start_date))
```

Get extra information to pass to the helper functions:
```{r}
ordered_names = names(test_set)

features = c("start_date","end_date","duration","start_station_number",
                  "start_station","bike_number", "member_type")
all_stops = names(test_set)[!(names(test_set) %in% features)]

```

Here's an example row:
```{r}
prediction_wrapper(test_set[500,], cbs_dt, all_stops, ordered_names)
```

Finally, out functions on the entire thing.

```{r}
library(plyr)
```

**Currently only running on part of the data.**

```{r}
final_predictions = adply(test_set[1:10,], 1, function(x) prediction_wrapper(x, cbs_dt, features, all_stops, ordered_names),
                           .progress = "text")
final_predictions
```


Save these results:
```{r}
#write.csv(final_predictions, "cbs_git-r-done.csv")
```

**This is writing the times incorrectly and adding row names...Also still has month and hour - we should drop those before stitching the rows together.**







